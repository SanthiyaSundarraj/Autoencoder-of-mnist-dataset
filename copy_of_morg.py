# -*- coding: utf-8 -*-
"""Copy of morg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u-tpi0opDP8SPRrcQrexGs0Wt863usq0
"""

from keras.models import Model
from keras.layers import Dense, Input
from keras.datasets import mnist
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import warnings
warnings.filterwarnings('ignore')

(x_train, _), (x_test, _) = mnist.load_data()
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))
print(x_train.shape)
print(x_test.shape)

batch_size = 128
nb_epoch = 15
img_rows, img_cols = 28, 28
nb_visible = img_rows * img_cols
nb_hidden1 = 392
nb_hidden2 = 196

input_img = Input(shape=(nb_visible,))
encoded1 = Dense(nb_hidden1, activation='relu')(input_img)
encoded2 = Dense(nb_hidden2, activation='relu')(encoded1)
decoded = Dense(nb_visible, activation='sigmoid')(encoded2)

autoencoder = Model(input=input_img, output=decoded)
autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')
autoencoder.summary()

autoencoder.fit(x_train, x_train, epochs=nb_epoch, batch_size=batch_size, shuffle=True, verbose=1,
                validation_data=(x_test, x_test))

evaluation = autoencoder.evaluate(x_test, x_test, batch_size=batch_size, verbose=1)
print('\nSummary: Loss over the test dataset: %.2f' % (evaluation))

decoded_imgs = autoencoder.predict(x_test)
n=15
plt.figure(figsize=(20, 4))
for i in range(n):
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

w = []
for layer in autoencoder.layers:
    weights = layer.get_weights()
    w.append(weights)

layer1 = np.array(w[2][0])
print("Shape of Hidden Layer",layer1.shape)
print("Visualization of Hidden Layer")
fig=plt.figure(figsize=(12, 12))
columns = 2
rows = 1
for i in range(1, columns*rows +1):
    fig.add_subplot(rows, columns, i)
    plt.imshow(layer1[:,i-1].reshape(28,14),cmap='gray')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# % matplotlib inline
# A thing of beauty is a joy forever
import seaborn as sns
sns.set()
sns.set_palette("husl")

C = 10 # We need 4 clusters
m = 2 # This is arbitrary
w = []
for layer in autoencoder.layers:
    weights = layer.get_weights()
    w.append(weights)

data = np.array(w[2][0])

print(data.shape)

def initialize_memberships(data, C):
    """Returns an NxC array of randomly assigned memberships."""
    N, _ = data.shape
    # Create a random array of shape (N, C)
    mems = np.random.random((N, C))   
    # Divide by the sum along axis 1 so that the numbers sum up to 1
    return mems / mems.sum(axis=1, keepdims=True)

def update_centres(data, memberships, m):
    """Returns new centres from given data points and memberships."""
    # Notice the equation for cj.
    # Isn't that plain old matrix multiplication?
    mems = memberships ** m
    # We first calculate  δij^m / ∑ δij^m as weights
    weights = mems / mems.sum(axis=0)
    # And then do the matrix multiplication
    return np.dot(weights.T, data)

def get_distances(data, centres):
    """Returns distances between each point and every centre."""
    # Remember Life without Loops? Let's build upon it.
    # Please read through the numpy docs on broadcasting,
    # particularly the last example about outer addition
    # https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html
    return np.sqrt(((data[:, None] - centres)**2).sum(axis=2))

def update_memberships(data, centres, m):
    """Update the memberships according to new centres."""
    # You might have got it by now. Broadcasting rocks!
    dist = get_distances(data, centres)
    dist = dist[:, None, :] / dist[:, :, None]
    dist = dist ** (2/(m-1))
    return 1/dist.sum(axis=1)

def fcmeans(data, C, m):
    """The Fuzzy C-Means algorithm."""
    memberships = initialize_memberships(data, C)
    initial_centres = update_centres(data, memberships, m)
    old_memberships = np.zeros_like(memberships)
    while not np.allclose(memberships, old_memberships):
        old_memberships = memberships
        centres = update_centres(data, memberships, m)
        memberships = update_memberships(data, centres, m)
    return memberships, centres, initial_centres

C = 10
m = 2
#data = np.load('data/acc.npy')

memberships, centres, initial_centres = fcmeans(data, C, m)

plt.subplot(121)
plt.scatter(data[:, 0], data[:, 1])
plt.scatter(initial_centres[:, 0], initial_centres[:, 1], c='r', s=100)
plt.subplot(122)
plt.scatter(data[:, 0], data[:, 1], c=memberships.argmax(axis=1))
plt.scatter(centres[:, 0], centres[:, 1], c='r', s=100)
plt.show()